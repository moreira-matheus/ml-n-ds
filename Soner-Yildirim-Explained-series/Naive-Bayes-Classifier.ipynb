{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on a post @ [Towards Data Science](https://towardsdatascience.com/naive-bayes-classifier-explained-50f9723571ed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier\n",
    "\n",
    "**Naive Bayes** is a supervised learning algorithm used for classification tasks.\n",
    "\n",
    "As other supervised learning algorithms, naive bayes uses features to make a prediction on a target variable.\n",
    "- The key difference is that naive bayes assumes that features are independent of each other and there is no correlation between features.\n",
    "- However, this is not the case in real life. This naive assumption of features being uncorrelated is the reason why this algorithm is called \"naive\".\n",
    "\n",
    "### Probability and conditional probability\n",
    "\n",
    "Probability simply means the likelihood of an event to occur and always takes a value between $0$ and $1$ ($0$ and $1$ inclusive).\n",
    "- The probability of event $A$ is denoted as $P(A)$ and calculated as the number of the desired outcome divided by the number of all outcomes.\n",
    "\n",
    "[Conditional probability](https://en.wikipedia.org/wiki/Conditional_probability) is the likelihood of an event A to occur given that another event that has a relation with event A has already occurred.\n",
    "- Suppose that we have 6 blue balls and 4 yellow balls placed in two boxes as seen below.\n",
    "- I ask you to randomly pick a ball.\n",
    "    - The probability of getting a blue ball is $6 / 10 = 0.6$.\n",
    "- What if I ask you to pick a ball from box A?\n",
    "    - The probability of picking a blue ball clearly decreases.\n",
    "    - The condition here is to pick from box A which clearly changes the probability of the event (picking a blue ball).\n",
    "- The probability of event A given that event B has occurred is denoted as $P(A|B)$.\n",
    "\n",
    "<br><center><img src=\"./IMG/box-a-box-b.png\" width=600></center><br>\n",
    "\n",
    "[Joint probability](https://en.wikipedia.org/wiki/Joint_probability_distribution) is the probability of two events occurring together.\n",
    "- It is denoted as $P(A \\wedge B)$.\n",
    "- For <u>independent events</u>, joint probability can be written as: $P(A \\wedge B) = P(A) \\cdot P(B)$.\n",
    "- In the case of <u>dependent events</u>, the previous equation is not valid.\n",
    "    - It should be slightly changed to hold for any two events: $P(A \\wedge B) = P(A) \\cdot P(B|A)$.\n",
    "        - The formula for independent events is a special case, in which $P(B|A) = P(B)$.\n",
    "\n",
    "### Bayes' theorem\n",
    "\n",
    "We will start with the fact that joint probability is commutative for any two events. That is: $P(A \\wedge B) = P(B \\wedge A)$.\n",
    "\n",
    "From that we know that $P(A \\wedge B) = P(A) \\cdot P(B|A)$ and $P(B \\wedge A) = P(B) \\cdot P(A|B)$.\n",
    "\n",
    "Then, $ P(A) \\cdot P(B|A) = P(B) \\cdot P(A|B) \\Rightarrow P(A|B) = \\displaystyle\\frac{P(A) P(B|A)}{P(B)}$ (Bayes' theorem).\n",
    "\n",
    "### Naive Bayes Classifier\n",
    "\n",
    "**Naive Bayes classifier** calculates the probability of a class given a set of $k$ feature values:\n",
    "\n",
    "$$\n",
    "P(y_i | x_{1,i}, x_{2,i}, ..., x_{k,i}) = \\displaystyle\\frac{P(x_{1,i}, x_{2,i}, ..., x_{k,i} | y_i) P(y_i)}{P(x_{1,i}, x_{2,i}, ..., x_{k,i})}\n",
    "$$\n",
    "\n",
    "$P(x_{1,i}, x_{2,i}, ..., x_{k,i} | y_i)$ means the probability of a specific combination of features given a class label.\n",
    "- To be able to calculate this, we need extremely large datasets to have an estimate on the probability distribution for all different combinations of feature values.\n",
    "- To overcome this issue, <u>Naive Bayes algorithm assumes that all features are independent of each other</u>.\n",
    "- Furthermore, denominator $P(x_{1,i}, x_{2,i}, ..., x_{k,i})$ can be removed to simplify the equation because it only normalizes the value of conditional probability of a class given an observation $P(y_i | x_{1,i}, x_{2,i}, ..., x_{k,i})$.\n",
    "\n",
    "The probability of a class is very simple to calculate: $P(y_i) = \\displaystyle\\frac{\\text{Num. of observation of class } y_i}{\\text{Num. of observations}}$.\n",
    "\n",
    "Under the assumption of features being independent: $P(x_{1,i}, x_{2,i}, ..., x_{k,i} | y_i) = P(x_{1,i} | y_i) \\cdot P(x_{2,i} | y_i) \\cdot ... \\cdot P(x_{k,i} | y_i)$.\n",
    "- The conditional probability for a single feature given the class label (i.e., $P(x_{1,i} | y_i)$) can be more easily estimated from the data.\n",
    "\n",
    "The algorithm needs to store probability distributions of features for each class independently.\n",
    "- For example, if there are $5$ classes and $10$ features, $50$ different probability distributions need to be stored.\n",
    "- The type of distributions depend on the characteristics of features:\n",
    "    - For binary features (Y/N, True/False, 0/1): Bernoulli distribution.\n",
    "    - For discrete features (i.e. word counts): Multinomial distribution.\n",
    "    - For continuous features: Gaussian (Normal) distribution.\n",
    "- It is common to name the Naive Bayes with the distribution of features (i.e. Gaussian Naive Bayes classifier).\n",
    "- For mixed type datasets, a different type of distribution may be required for different features.\n",
    "\n",
    "##### Pros and Cons of Naive Bayes Algorithm\n",
    "\n",
    "Pros:\n",
    "\n",
    "- The assumption that all features are independent makes naive bayes algorithm very fast compared to complicated algorithms. In some cases, speed is preferred over higher accuracy.\n",
    "- It works well with high-dimensional data such as text classification, email spam detection.\n",
    "\n",
    "Cons:\n",
    "\n",
    "- The assumption that all features are independent is not usually the case in real life so it makes naive bayes algorithm less accurate than complicated algorithms. Speed comes at a cost!\n",
    "\n",
    "### Scikit-learn Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BigData\\AppData\\Local\\Continuum\\anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\BigData\\AppData\\Local\\Continuum\\anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GaussianNB().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breast cancer dataset\n",
      "Accuracy of GaussinNB classifier on training set: 0.95\n",
      "Accuracy of GaussinNB classifier on test set: 0.94\n"
     ]
    }
   ],
   "source": [
    "print('Breast cancer dataset')\n",
    "print('Accuracy of GaussinNB classifier on training set: {:.2f}'.format(clf.score(X_train, y_train)))\n",
    "print('Accuracy of GaussinNB classifier on test set: {:.2f}'.format(clf.score(X_test, y_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
